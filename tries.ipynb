{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-18.1.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.15.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "   ---------------------------------------- 0.0/179.3 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/179.3 kB ? eta -:--:--\n",
      "   -------- ------------------------------ 41.0/179.3 kB 393.8 kB/s eta 0:00:01\n",
      "   --------------- ----------------------- 71.7/179.3 kB 563.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 174.1/179.3 kB 952.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 179.3/179.3 kB 899.0 kB/s eta 0:00:00\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow-18.1.0-cp312-cp312-win_amd64.whl (25.1 MB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "Successfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 pyarrow-18.1.0 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "from reuters21578 import Reuters21578\n",
    "import torch\n",
    "\n",
    "! pip install transformers\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reuters21578 import Reuters21578\n",
    "\n",
    "# Create an instance of the dataset with a specific configuration\n",
    "dataset = Reuters21578(config_name='ModApte')\n",
    "\n",
    "# Download and prepare the dataset\n",
    "dataset.download_and_prepare()\n",
    "\n",
    "# Load the dataset\n",
    "datasets = dataset.as_dataset()\n",
    "\n",
    "# Access the train split\n",
    "train_dataset = datasets['train']\n",
    "\n",
    "# Print the labels for the first few examples\n",
    "topics_count_dict = {}\n",
    "for example in train_dataset:\n",
    "    for topic in example['topics']:\n",
    "        if topic not in topics_count_dict:\n",
    "            topics_count_dict[topic] = 0\n",
    "        topics_count_dict[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cocoa']\n",
      "['grain', 'wheat', 'corn', 'barley', 'oat', 'sorghum']\n",
      "['veg-oil', 'linseed', 'lin-oil', 'soy-oil', 'sun-oil', 'soybean', 'oilseed', 'corn', 'sunseed', 'grain', 'sorghum', 'wheat']\n",
      "[]\n",
      "['earn']\n",
      "['acq']\n",
      "['earn']\n",
      "['earn', 'acq']\n",
      "['earn']\n",
      "['earn']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_dataset[i]['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('earn', 2877)\n",
      "('acq', 1650)\n",
      "('money-fx', 538)\n",
      "('grain', 433)\n",
      "('crude', 389)\n",
      "('trade', 369)\n",
      "('interest', 347)\n",
      "('wheat', 212)\n",
      "('ship', 197)\n",
      "('corn', 182)\n",
      "('money-supply', 140)\n",
      "('dlr', 131)\n",
      "('sugar', 126)\n",
      "('oilseed', 124)\n",
      "('coffee', 111)\n",
      "('gnp', 101)\n",
      "('gold', 94)\n",
      "('veg-oil', 87)\n",
      "('soybean', 78)\n",
      "('livestock', 75)\n",
      "('nat-gas', 75)\n",
      "('bop', 75)\n",
      "('cpi', 69)\n",
      "('cocoa', 55)\n",
      "('reserves', 55)\n",
      "('carcass', 50)\n",
      "('copper', 47)\n",
      "('jobs', 46)\n",
      "('yen', 45)\n",
      "('ipi', 41)\n",
      "('iron-steel', 40)\n",
      "('cotton', 39)\n",
      "('barley', 37)\n",
      "('rubber', 37)\n",
      "('gas', 37)\n",
      "('rice', 35)\n",
      "('alum', 35)\n",
      "('meal-feed', 30)\n",
      "('palm-oil', 30)\n",
      "('sorghum', 24)\n",
      "('retail', 23)\n",
      "('silver', 21)\n",
      "('zinc', 21)\n",
      "('pet-chem', 20)\n",
      "('wpi', 19)\n",
      "('tin', 18)\n",
      "('rapeseed', 18)\n",
      "('stg', 17)\n",
      "('housing', 16)\n",
      "('strategic-metal', 16)\n",
      "('hog', 16)\n",
      "('orange', 16)\n",
      "('lead', 15)\n",
      "('soy-oil', 14)\n",
      "('heat', 14)\n",
      "('soy-meal', 13)\n",
      "('fuel', 13)\n",
      "('lei', 12)\n",
      "('sunseed', 11)\n",
      "('lumber', 10)\n",
      "('dmk', 10)\n",
      "('tea', 9)\n",
      "('income', 9)\n",
      "('oat', 8)\n",
      "('nickel', 8)\n",
      "('l-cattle', 6)\n",
      "('sun-oil', 5)\n",
      "('platinum', 5)\n",
      "('rape-oil', 5)\n",
      "('groundnut', 5)\n",
      "('instal-debt', 5)\n",
      "('inventories', 5)\n",
      "('plywood', 4)\n",
      "('jet', 4)\n",
      "('coconut-oil', 4)\n",
      "('austdlr', 4)\n",
      "('coconut', 4)\n",
      "('tapioca', 3)\n",
      "('propane', 3)\n",
      "('saudriyal', 3)\n",
      "('potato', 3)\n",
      "('can', 3)\n",
      "('cpu', 3)\n",
      "('pork-belly', 3)\n",
      "('linseed', 2)\n",
      "('copra-cake', 2)\n",
      "('palmkernel', 2)\n",
      "('cornglutenfeed', 2)\n",
      "('wool', 2)\n",
      "('fishmeal', 2)\n",
      "('palladium', 2)\n",
      "('dfl', 2)\n",
      "('naphtha', 2)\n",
      "('nzdlr', 2)\n",
      "('rand', 2)\n",
      "('lin-oil', 1)\n",
      "('rye', 1)\n",
      "('red-bean', 1)\n",
      "('groundnut-oil', 1)\n",
      "('citruspulp', 1)\n"
     ]
    }
   ],
   "source": [
    "# print top 10 topics\n",
    "\n",
    "sorted_topics = sorted(topics_count_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(100):\n",
    "    print(sorted_topics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size: 10, Cosine similarity mean: 0.7154720425605774, Cosine similarity max: 0.9022217988967896\n",
      "Window size: 30, Cosine similarity mean: 0.7808124423027039, Cosine similarity max: 0.8669080138206482\n",
      "Window size: 50, Cosine similarity mean: 0.7851669192314148, Cosine similarity max: 0.878635585308075\n",
      "Window size: 100, Cosine similarity mean: 0.8139405846595764, Cosine similarity max: 0.9040043354034424\n",
      "Window size: inf, Cosine similarity mean: 0.9119062423706055, Cosine similarity max: 0.9119062423706055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def split_doc_by_window(doc, window_size):\n",
    "    # Initialize the BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize the document\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    current_chunk_size = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        chunk.append(token)\n",
    "        current_chunk_size += 1\n",
    "\n",
    "        if token == '.' and current_chunk_size >= window_size:\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = []\n",
    "            current_chunk_size = 0\n",
    "\n",
    "    if len(chunk) > 0:\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def encode_chunks(chunks):\n",
    "    encoding = tokenizer.batch_encode_plus( chunks,# List of input texts\n",
    "    padding=True,              # Pad to the maximum sequence length\n",
    "    truncation=True,           # Truncate to the maximum sequence length if necessary\n",
    "    return_tensors='pt',      # Return PyTorch tensors\n",
    "    add_special_tokens=True    # Add special tokens CLS and SEP\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids']  # Tokenized and encoded sentences\n",
    "    attention_mask = encoding['attention_mask']  # Attention mask\n",
    "\n",
    "    # Generate embeddings using BERT model\n",
    "\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        chunks_embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    return chunks_embedding\n",
    "\n",
    "def split_and_encode_doc(doc, window_size):\n",
    "    chunks = split_doc_by_window(doc, window_size)\n",
    "    chunks_embedding = encode_chunks(chunks)\n",
    "\n",
    "    return chunks_embedding\n",
    "\n",
    "def get_cos_sim_mat(embedding_1, embedding_2):\n",
    "    cos_sim_matrix = embedding_1 @ embedding_2.T\n",
    "\n",
    "    norm_1 = torch.norm(embedding_1, dim=1)\n",
    "    norm_2 = torch.norm(embedding_2, dim=1)\n",
    "\n",
    "    norm_mult_div = norm_1.reshape(-1, 1) @ norm_2.reshape(-1, 1).T\n",
    "\n",
    "    cos_sim_matrix = cos_sim_matrix / norm_mult_div\n",
    "\n",
    "    return cos_sim_matrix\n",
    "\n",
    "\n",
    "windows = [10, 30, 50, 100, float('inf')]\n",
    "doc_1 = train_dataset[1]['text']\n",
    "doc_2 = train_dataset[2]['text']\n",
    "for window in windows:\n",
    "    chunks_embedding_1 = split_and_encode_doc(doc_1, window)\n",
    "    chunks_embedding_2 = split_and_encode_doc(doc_2, window)\n",
    "\n",
    "    cos_sim_matrix = get_cos_sim_mat(chunks_embedding_1, chunks_embedding_2)\n",
    "    print(f'Window size: {window}, Cosine similarity mean: {cos_sim_matrix.mean()}, Cosine similarity max: {cos_sim_matrix.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def embed(self, chunks):\n",
    "        pass\n",
    "\n",
    "class BertEmbedder(Embedder):\n",
    "    def __init__(self):\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def embed(self, chunks):\n",
    "        encodings = self.tokenizer.batch_encode_plus(chunks, padding=True, truncation=True, return_tensors='pt', add_special_tokens=True)\n",
    "        input_ids = encodings['input_ids']\n",
    "        attention_mask = encodings['attention_mask']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "class TFIDFEmbedder(Embedder):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def embed(self, chunks):\n",
    "        embeddings = self.vectorizer.fit_transform(chunks)\n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "\n",
    "class CustomClusteringModel:\n",
    "    def __init__(self, window_size):\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def data(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def fit(self, embbeders, weights = 'inv_sqrt_len'):\n",
    "        if weights == 'inv_sqrt_len':\n",
    "            weights_func = self._inv_sqrt_len_weights\n",
    "\n",
    "        for doc in self.dataset:\n",
    "            chunks = self._split_doc_by_window(doc['text'], self.window_size)\n",
    "            embedding = []\n",
    "            for embedder in embbeders:\n",
    "                embed = embedder.embed(chunks)\n",
    "                weights = weights_func(embed)\n",
    "                embedding.append(embed * weights)\n",
    "\n",
    "            self.embeddings.append(embedding)\n",
    "\n",
    "    def cluster(self, n, method='kmeans'):\n",
    "        if method == 'kmeans':\n",
    "            self.kmeans = KMeans(n_clusters=n, random_state=0).fit(self.embeddings)\n",
    "            self.labels = self.kmeans.labels_\n",
    "        else:\n",
    "            raise ValueError(f'Clustering method {method} is not supported')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _split_doc_by_window(doc, window_size):\n",
    "        # Initialize the BERT tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Tokenize the document\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        chunks = []\n",
    "        chunk = []\n",
    "        current_chunk_size = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            chunk.append(token)\n",
    "            current_chunk_size += 1\n",
    "\n",
    "            if token == '.' and current_chunk_size >= window_size:\n",
    "                chunks.append(' '.join(chunk))\n",
    "                chunk = []\n",
    "                current_chunk_size = 0\n",
    "\n",
    "        if len(chunk) > 0:\n",
    "            chunks.append(' '.join(chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _inv_sqrt_len_weights(self, embeddings):\n",
    "        return 1/(len(embeddings[0])**0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
