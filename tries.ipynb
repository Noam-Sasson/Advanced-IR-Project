{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.15.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets\n",
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "from reuters21578 import Reuters21578\n",
    "import torch\n",
    "\n",
    "! pip install transformers\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from reuters21578 import Reuters21578\n",
    "\n",
    "# Create an instance of the dataset with a specific configuration\n",
    "dataset = Reuters21578(config_name='ModApte')\n",
    "\n",
    "# Download and prepare the dataset\n",
    "dataset.download_and_prepare()\n",
    "\n",
    "# Load the dataset\n",
    "datasets = dataset.as_dataset()\n",
    "\n",
    "# Access the train split\n",
    "train_dataset = datasets['train']\n",
    "\n",
    "# Print the labels for the first few examples\n",
    "topics_count_dict = {}\n",
    "for example in train_dataset:\n",
    "    for topic in example['topics']:\n",
    "        if topic not in topics_count_dict:\n",
    "            topics_count_dict[topic] = 0\n",
    "        topics_count_dict[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9603\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cocoa']\n",
      "['grain', 'wheat', 'corn', 'barley', 'oat', 'sorghum']\n",
      "['veg-oil', 'linseed', 'lin-oil', 'soy-oil', 'sun-oil', 'soybean', 'oilseed', 'corn', 'sunseed', 'grain', 'sorghum', 'wheat']\n",
      "[]\n",
      "['earn']\n",
      "['acq']\n",
      "['earn']\n",
      "['earn', 'acq']\n",
      "['earn']\n",
      "['earn']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_dataset[i]['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U.S. Agriculture Department\n",
      "reported the farmer-owned reserve national five-day average\n",
      "price through February 25 as follows (Dlrs/Bu-Sorghum Cwt) -\n",
      "         Natl   Loan           Release   Call\n",
      "         Avge   Rate-X  Level    Price  Price\n",
      " Wheat   2.55   2.40       IV     4.65     --\n",
      "                            V     4.65     --\n",
      "                           VI     4.45     --\n",
      " Corn    1.35   1.92       IV     3.15   3.15\n",
      "                            V     3.25     --\n",
      " X - 1986 Rates.\n",
      "\n",
      "          Natl   Loan          Release   Call\n",
      "          Avge   Rate-X  Level   Price  Price\n",
      " Oats     1.24   0.99        V    1.65    -- \n",
      " Barley   n.a.   1.56       IV    2.55   2.55\n",
      "                             V    2.65    -- \n",
      " Sorghum  2.34   3.25-Y     IV    5.36   5.36\n",
      "                             V    5.54    -- \n",
      "    Reserves I, II and III have matured. Level IV reflects\n",
      "grain entered after Oct 6, 1981 for feedgrain and after July\n",
      "23, 1981 for wheat. Level V wheat/barley after 5/14/82,\n",
      "corn/sorghum after 7/1/82. Level VI covers wheat entered after\n",
      "January 19, 1984.  X-1986 rates. Y-dlrs per CWT (100 lbs).\n",
      "n.a.-not available.\n",
      " Reuter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('earn', 2877)\n",
      "('acq', 1650)\n",
      "('money-fx', 538)\n",
      "('grain', 433)\n",
      "('crude', 389)\n",
      "('trade', 369)\n",
      "('interest', 347)\n",
      "('wheat', 212)\n",
      "('ship', 197)\n",
      "('corn', 182)\n",
      "('money-supply', 140)\n",
      "('dlr', 131)\n",
      "('sugar', 126)\n",
      "('oilseed', 124)\n",
      "('coffee', 111)\n",
      "('gnp', 101)\n",
      "('gold', 94)\n",
      "('veg-oil', 87)\n",
      "('soybean', 78)\n",
      "('livestock', 75)\n",
      "('nat-gas', 75)\n",
      "('bop', 75)\n",
      "('cpi', 69)\n",
      "('cocoa', 55)\n",
      "('reserves', 55)\n",
      "('carcass', 50)\n",
      "('copper', 47)\n",
      "('jobs', 46)\n",
      "('yen', 45)\n",
      "('ipi', 41)\n",
      "('iron-steel', 40)\n",
      "('cotton', 39)\n",
      "('barley', 37)\n",
      "('rubber', 37)\n",
      "('gas', 37)\n",
      "('rice', 35)\n",
      "('alum', 35)\n",
      "('meal-feed', 30)\n",
      "('palm-oil', 30)\n",
      "('sorghum', 24)\n",
      "('retail', 23)\n",
      "('silver', 21)\n",
      "('zinc', 21)\n",
      "('pet-chem', 20)\n",
      "('wpi', 19)\n",
      "('tin', 18)\n",
      "('rapeseed', 18)\n",
      "('stg', 17)\n",
      "('housing', 16)\n",
      "('strategic-metal', 16)\n",
      "('hog', 16)\n",
      "('orange', 16)\n",
      "('lead', 15)\n",
      "('soy-oil', 14)\n",
      "('heat', 14)\n",
      "('soy-meal', 13)\n",
      "('fuel', 13)\n",
      "('lei', 12)\n",
      "('sunseed', 11)\n",
      "('lumber', 10)\n",
      "('dmk', 10)\n",
      "('tea', 9)\n",
      "('income', 9)\n",
      "('oat', 8)\n",
      "('nickel', 8)\n",
      "('l-cattle', 6)\n",
      "('sun-oil', 5)\n",
      "('platinum', 5)\n",
      "('rape-oil', 5)\n",
      "('groundnut', 5)\n",
      "('instal-debt', 5)\n",
      "('inventories', 5)\n",
      "('plywood', 4)\n",
      "('jet', 4)\n",
      "('coconut-oil', 4)\n",
      "('austdlr', 4)\n",
      "('coconut', 4)\n",
      "('tapioca', 3)\n",
      "('propane', 3)\n",
      "('saudriyal', 3)\n",
      "('potato', 3)\n",
      "('can', 3)\n",
      "('cpu', 3)\n",
      "('pork-belly', 3)\n",
      "('linseed', 2)\n",
      "('copra-cake', 2)\n",
      "('palmkernel', 2)\n",
      "('cornglutenfeed', 2)\n",
      "('wool', 2)\n",
      "('fishmeal', 2)\n",
      "('palladium', 2)\n",
      "('dfl', 2)\n",
      "('naphtha', 2)\n",
      "('nzdlr', 2)\n",
      "('rand', 2)\n",
      "('lin-oil', 1)\n",
      "('rye', 1)\n",
      "('red-bean', 1)\n",
      "('groundnut-oil', 1)\n",
      "('citruspulp', 1)\n"
     ]
    }
   ],
   "source": [
    "# print top 10 topics\n",
    "\n",
    "sorted_topics = sorted(topics_count_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(100):\n",
    "    print(sorted_topics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size: 10, Cosine similarity mean: 0.7154720425605774, Cosine similarity max: 0.9022217988967896\n",
      "Window size: 30, Cosine similarity mean: 0.7808124423027039, Cosine similarity max: 0.8669080138206482\n",
      "Window size: 50, Cosine similarity mean: 0.7851669192314148, Cosine similarity max: 0.878635585308075\n",
      "Window size: 100, Cosine similarity mean: 0.8139405846595764, Cosine similarity max: 0.9040043354034424\n",
      "Window size: inf, Cosine similarity mean: 0.9119062423706055, Cosine similarity max: 0.9119062423706055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def split_doc_by_window(doc, window_size):\n",
    "    # Initialize the BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize the document\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    current_chunk_size = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        chunk.append(token)\n",
    "        current_chunk_size += 1\n",
    "\n",
    "        if token == '.' and current_chunk_size >= window_size:\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = []\n",
    "            current_chunk_size = 0\n",
    "\n",
    "    if len(chunk) > 0:\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def encode_chunks(chunks):\n",
    "    encoding = tokenizer.batch_encode_plus( chunks,# List of input texts\n",
    "    padding=True,              # Pad to the maximum sequence length\n",
    "    truncation=True,           # Truncate to the maximum sequence length if necessary\n",
    "    return_tensors='pt',      # Return PyTorch tensors\n",
    "    add_special_tokens=True    # Add special tokens CLS and SEP\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids']  # Tokenized and encoded sentences\n",
    "    attention_mask = encoding['attention_mask']  # Attention mask\n",
    "\n",
    "    # Generate embeddings using BERT model\n",
    "\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        chunks_embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    return chunks_embedding\n",
    "\n",
    "def split_and_encode_doc(doc, window_size):\n",
    "    chunks = split_doc_by_window(doc, window_size)\n",
    "    chunks_embedding = encode_chunks(chunks)\n",
    "\n",
    "    return chunks_embedding\n",
    "\n",
    "def get_cos_sim_mat(embedding_1, embedding_2):\n",
    "    cos_sim_matrix = embedding_1 @ embedding_2.T\n",
    "\n",
    "    norm_1 = torch.norm(embedding_1, dim=1)\n",
    "    norm_2 = torch.norm(embedding_2, dim=1)\n",
    "\n",
    "    norm_mult_div = norm_1.reshape(-1, 1) @ norm_2.reshape(-1, 1).T\n",
    "\n",
    "    cos_sim_matrix = cos_sim_matrix / norm_mult_div\n",
    "\n",
    "    return cos_sim_matrix\n",
    "\n",
    "\n",
    "windows = [10, 30, 50, 100, float('inf')]\n",
    "doc_1 = train_dataset[1]['text']\n",
    "doc_2 = train_dataset[2]['text']\n",
    "for window in windows:\n",
    "    chunks_embedding_1 = split_and_encode_doc(doc_1, window)\n",
    "    chunks_embedding_2 = split_and_encode_doc(doc_2, window)\n",
    "\n",
    "    cos_sim_matrix = get_cos_sim_mat(chunks_embedding_1, chunks_embedding_2)\n",
    "    print(f'Window size: {window}, Cosine similarity mean: {cos_sim_matrix.mean()}, Cosine similarity max: {cos_sim_matrix.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def embed(self, chunks)->torch.Tensor:\n",
    "        pass\n",
    "\n",
    "class BertEmbedder(Embedder):\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased').to(self.device)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        print(f'BertEmbedder Using device: {self.device}')\n",
    "\n",
    "    def embed(self, chunks):\n",
    "        encodings = self.tokenizer.batch_encode_plus(chunks, padding=True, truncation=True, return_tensors='pt', add_special_tokens=True).to(self.device)\n",
    "        input_ids = encodings['input_ids'].to(self.device)\n",
    "        attention_mask = encodings['attention_mask'].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return embeddings.to('cpu')\n",
    "    \n",
    "class TFIDFEmbedder(Embedder):\n",
    "    def __init__(self, corpus):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.vectorizer.fit(corpus)\n",
    "\n",
    "    def embed(self, chunks):\n",
    "        embeddings = self.vectorizer.transform(chunks).toarray()\n",
    "        return torch.tensor(embeddings)\n",
    "        \n",
    "class CustomClusteringModel:\n",
    "    def __init__(self, window_size):\n",
    "        self.window_size = window_size\n",
    "        self.embeddings_per_doc_list = []\n",
    "        self.combined_embeddings_list = []\n",
    "        self.labels = []\n",
    "\n",
    "    def data(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    # def fit(self, embbeders):\n",
    "    #     for doc in tqdm(self.dataset, desc=\"Processing documents\"):\n",
    "    #         chunks = self._split_doc_by_window(doc['text'], self.window_size)\n",
    "    #         embeddings_ = []\n",
    "\n",
    "    #         if len(chunks) != 0: #somehow there are empty documents\n",
    "    #             for embedder in embbeders:\n",
    "    #                 embed = embedder.embed(chunks)\n",
    "    #                 embeddings_.append(embed)\n",
    "\n",
    "    #         self.embeddings_per_doc_list.append(embeddings_)\n",
    "    #         self.labels.append(doc['topics'])\n",
    "\n",
    "    def fit(self, embedders):\n",
    "        def _process_doc(doc):\n",
    "            try:\n",
    "                chunks = self._split_doc_by_window(doc['text'], self.window_size)\n",
    "                embeddings_ = []\n",
    "\n",
    "                if len(chunks) != 0:  # somehow there are empty documents\n",
    "                    for embedder in embedders:\n",
    "                        embed = embedder.embed(chunks)\n",
    "                        embeddings_.append(embed)\n",
    "\n",
    "                return embeddings_, doc['topics']\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing document: {e}\")\n",
    "                return None, None\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(_process_doc, doc) for doc in self.dataset]\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing documents\"):\n",
    "                embeddings_, topics = future.result()\n",
    "                self.embeddings_per_doc_list.append(embeddings_)\n",
    "                self.labels.append(topics)\n",
    "    \n",
    "    def combine_embeddings_per_doc(self, weights_type='mean'):\n",
    "        new_labels = []\n",
    "        for embeddings_, label in zip(self.embeddings_per_doc_list, self.labels):\n",
    "            if len(embeddings_) != 0: # remember that there are empty documents\n",
    "                if weights_type == 'mean':\n",
    "                    combined_embedding = torch.cat(embeddings_, dim=1).mean(dim=0)\n",
    "                else:\n",
    "                    raise ValueError(f'Weights type {weights_type} is not supported')\n",
    "                self.combined_embeddings_list.append(combined_embedding)\n",
    "                new_labels.append(label)\n",
    "\n",
    "        self.labels = new_labels\n",
    "        \n",
    "    def cluster(self, n, method='kmeans'):\n",
    "        if method == 'kmeans':\n",
    "            self.kmeans = KMeans(n_clusters=n, random_state=0).fit(self.combined_embeddings_list)\n",
    "        else:\n",
    "            raise ValueError(f'Clustering method {method} is not supported')\n",
    "    \n",
    "\n",
    "    def _split_doc_by_window(self, doc, window_size):\n",
    "        # Initialize the BERT tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Tokenize the document\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        chunks = []\n",
    "        chunk = []\n",
    "        current_chunk_size = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            chunk.append(token)\n",
    "            current_chunk_size += 1\n",
    "\n",
    "            if token == '.' and current_chunk_size >= window_size:\n",
    "                chunks.append(' '.join(chunk))\n",
    "                chunk = []\n",
    "                current_chunk_size = 0\n",
    "\n",
    "        if len(chunk) > 0:\n",
    "            chunks.append(' '.join(chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _get_cos_sim_mat(self, embedding_1, embedding_2):\n",
    "        cos_sim_matrix = embedding_1 @ embedding_2.T\n",
    "\n",
    "        norm_1 = torch.norm(embedding_1, dim=1)\n",
    "        norm_2 = torch.norm(embedding_2, dim=1)\n",
    "\n",
    "        norm_mult_div = norm_1.reshape(-1, 1) @ norm_2.reshape(-1, 1).T\n",
    "\n",
    "        cos_sim_matrix = cos_sim_matrix / norm_mult_div\n",
    "\n",
    "        return cos_sim_matrix\n",
    "\n",
    "    def _inv_sqrt_len_weights(self, embeddings):\n",
    "        return 1/(len(embeddings[0])**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertEmbedder Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 100/100 [00:16<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "subset_dataset = train_dataset.select(range(100))\n",
    "\n",
    "train_corpus = [doc['text'] for doc in subset_dataset]\n",
    "\n",
    "bert_embedder = BertEmbedder()\n",
    "tfidf_embedder = TFIDFEmbedder(train_corpus)\n",
    "\n",
    "embedders = [bert_embedder, tfidf_embedder]\n",
    "\n",
    "general_clustering_model = CustomClusteringModel(window_size=10)\n",
    "\n",
    "\n",
    "\n",
    "general_clustering_model.data(subset_dataset)\n",
    "\n",
    "general_clustering_model.fit(embedders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "2221\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(general_clustering_model.embeddings_per_doc_list))\n",
    "print(len(general_clustering_model.embeddings_per_doc_list[0][1][0]))\n",
    "print(len(general_clustering_model.embeddings_per_doc_list[0][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_clustering_model.combine_embeddings_per_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 768]) torch.Size([16, 2221])\n",
      "torch.Size([2989])\n"
     ]
    }
   ],
   "source": [
    "print(general_clustering_model.embeddings_per_doc_list[0][0].shape, general_clustering_model.embeddings_per_doc_list[0][1].shape)\n",
    "print(general_clustering_model.combined_embeddings_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset top 10 topics\n",
    "kmans_model = general_clustering_model.cluster(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'kmeans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_topics)\n\u001b[0;32m     17\u001b[0m kmeans_model \u001b[38;5;241m=\u001b[39m general_clustering_model\u001b[38;5;241m.\u001b[39mcluster(n)\n\u001b[1;32m---> 18\u001b[0m kmeans_labels \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkmeans\u001b[49m\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_topics))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# topics statistics for each cluster\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'kmeans'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Convert combined_embeddings_list to a NumPy array\n",
    "combined_embeddings_array = np.array([embedding.numpy() for embedding in general_clustering_model.combined_embeddings_list])\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "tsne_embeddings = tsne.fit_transform(combined_embeddings_array)\n",
    "\n",
    "all_topics = set(l for sublist in general_clustering_model.labels for l in sublist)\n",
    "\n",
    "# Perform KMeans clustering\n",
    "n = len(all_topics)\n",
    "kmeans_model = general_clustering_model.cluster(n)\n",
    "kmeans_labels = kmeans_model.kmeans.labels_\n",
    "\n",
    "print(len(all_topics))\n",
    "\n",
    "\n",
    "# topics statistics for each cluster\n",
    "topic_to_index = {topic: i for i, topic in enumerate(all_topics)}\n",
    "index_to_topic = {i: topic for i, topic in enumerate(all_topics)}\n",
    "\n",
    "cluster_topic_counts = {i: {topic: 0 for topic in all_topics} for i in range(n)}\n",
    "\n",
    "for cluster_label, topics in zip(kmeans_labels, general_clustering_model.labels):\n",
    "    for topic in topics:\n",
    "        cluster_topic_counts[int(cluster_label)][topic] += 1\n",
    "    \n",
    "# print confusion matrix using matplotlib\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.matshow(np.array([list(cluster_topic_counts[i].values()) for i in range(10)], dtype=float), cmap='viridis')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(range(n))\n",
    "ax.set_xticklabels(all_topics, rotation=90)\n",
    "ax.set_yticks(range(n))\n",
    "ax.set_yticklabels(range(n))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the t-SNE embeddings\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=kmeans_labels, cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.title('t-SNE visualization of document embeddings')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
